library(DiceKriging)
library(lhs)



# GP - Lower Confidence Bound
lcb <- function(x,gp,beta=1) {
  stopifnot(is.vector(x))
  x = t(x)
  pred = predict(gp,data.frame(x),"UK",checkNames=F)
  lcb = pred$mean+sqrt(beta)*pred$sd
}

# GP's predictive mean
gp.mu <- function(x,gp) {
  stopifnot(is.vector(x))
  x = t(x)
  pred = predict(gp,data.frame(x),"UK",checkNames=F)
  mu = pred$mean
} 

# GP's predictive uncertainty
gp.sd <- function(x,gp) {
  stopifnot(is.vector(x))
  x = t(x)
  pred = predict(gp,data.frame(x),"UK",checkNames=F)
  max.sd = pred$sd
}

# Inverse Weighted Distance (model-free uncertainty)
iwd <- function(x,X) {
  D = as.matrix(dist(rbind(x,X)))
  D = D[1,-1]
  w = exp(-D^2)/(D)^2
  if(length(which(D<10^-8))>0) { 
    z=0
  } else {
    z = (2/pi) *atan( 1/sum(w) )
  }
  iwd = z
}

# function assessing if points X belongs to the hypercube
# with side L and centered in xc
inHypercube <- function(X,xc,L) {
  stopifnot( is.matrix(X) & is.vector(xc) )
  counter = 0
  for( i in 1:nrow(X) ) {
    ixs = which( (xc+L/2) - X[i,]<0 | X[i,] - (xc-L/2) <0 )
    if( length(ixs)>0 )
      counter= counter+1
  }
  inHypercube = counter
}


# Probability of Improvement as defined in "PI is back!"
PI <- function(x,gp) {
  stopifnot(is.vector(x))
  x = t(x)
  pred = predict(gp,data.frame(x),"UK",checkNames=F)
  I = min(gp@y) - pred$mean
  
  # cdf
  PI = pnorm( I/pred$sd )
}


# Expected Improvement as defined in "PI is back!"
EI <- function(x,gp,dgt) {
  stopifnot(is.vector(x))
  x = t(x)
  pred = predict(gp,data.frame(x),"UK",checkNames=F)
  if(pred$sd<10^-dgt) {
    res = 0
  } else {
    I = min(gp@y) - pred$mean
    res = I * pnorm( I/pred$sd ) + pred$sd * dnorm( I/pred$sd )
  }
  
  # cdf
  EI = res
}





# GP-LCB with Srinivas's scheduling for beta (Theorem 1)
next.Srinivas_1 <- function(gp,dgt,delta) {
  beta = 2 * log10( (10^(dgt*gp@d)) * ((pi*length(gp@y))^2) / (6*delta)  )
  beta =  round(beta,dgt)
  
  # x0s = round(maximinLHS(100*gp@d,gp@d),dgt)
  # y0s = apply(x0s,1,lcb,gp=gp,beta=beta)
  # x0s = x0s[order(y0s)[1:5],]
  # for( i in 1:nrow(x0s) ) {
  #   x0 = x0s[i,]
  #   res = optim(par=x0,fn=lcb,gr=NULL,method="L-BFGS-B",lower=0,upper=1,
  #               control=list(trace=0,factr=10^-dgt),
  #               gp=gp,beta=beta)
  #   if( i==1 || res$value<res.best$value )
  #     res.best = res
  # }
  
  
  xc = xc.prev = rep(0.5,gp@d)
  notImproved = 0
  len = 1
  while( notImproved<5 && len>=0.01 ) {
    x0s = round(maximinLHS(100*gp@d,gp@d),dgt) * len + xc-len/2
    x0s = rbind(xc,x0s)
    x0s[which(x0s<0)] = 0; x0s[which(x0s>1)] = 1
    y0s = round(apply(x0s,1,lcb,gp=gp,beta=beta),dgt)
    xc = x0s[which.min(y0s),]
    len = 3 * len/4
    if( all(xc==xc.prev) ) {
      notImproved = notImproved+1
    } else {
      xc.prev = xc
      notImproved = 0
    }
  }
  res.best = list(par=xc)
  
  next.Srinivas = list(par=res.best$par,af.info=paste0("beta=",beta),af.note=NA)
}

# GP-LCB with Srinivas's scheduling for beta (Theorem 2+'empirical workaround' on beta)
next.Srinivas_2 <- function(gp,dgt,delta) {
  
  # ----------------------------------
  # fixed according to paper.....
  r=1
  a=b=1
  # ----------------------------------
  
  beta = 2*log10( ((pi*length(gp@y))^2)/(3*delta) ) + (2*gp@d) * log10(sqrt(log(4*gp@d*a/delta))*gp@d*b*r*length(gp@y)^2)
  beta = beta/5 # <-- empirical workaround adopted in Srinivas
  beta =  round(beta,dgt)
  
  # x0s = round(maximinLHS(100*gp@d,gp@d),dgt)
  # y0s = apply(x0s,1,lcb,gp=gp,beta=beta)
  # x0s = x0s[order(y0s)[1:5],]
  # for( i in 1:nrow(x0s) ) {
  #   x0 = x0s[i,]
  #   res = optim(par=x0,fn=lcb,gr=NULL,method="L-BFGS-B",lower=0,upper=1,
  #               control=list(trace=0,factr=10^-dgt),
  #               gp=gp,beta=beta)
  #   if( i==1 || res$value<res.best$value )
  #     res.best = res
  # }
  
  xc = xc.prev = rep(0.5,gp@d)
  notImproved = 0
  len = 1
  while( notImproved<5 && len>=0.01 ) {
    x0s = round(maximinLHS(100*gp@d,gp@d),dgt) * len + xc-len/2
    x0s = rbind(xc,x0s)
    x0s[which(x0s<0)] = 0; x0s[which(x0s>1)] = 1
    y0s = round(apply(x0s,1,lcb,gp=gp,beta=beta),dgt)
    xc = x0s[which.min(y0s),]
    len = 3 * len/4
    if( all(xc==xc.prev) ) {
      notImproved = notImproved+1
    } else {
      xc.prev = xc
      notImproved = 0
    }
  }
  res.best = list(par=xc)
  
  next.Srinivas = list(par=res.best$par,af.info=paste0("beta=",beta),af.note=NA)
}

# Epsilon-greedy LCB / Random Search
next.epsRS <- function(gp,epsilon) {
  if( runif(1,0,1)<epsilon ) {
    info = "random"
    par = round(maximinLHS(1,gp@d),dgt)
  } else {
    # x0s = round(maximinLHS(100*gp@d,gp@d),dgt)
    # y0s = apply(x0s,1,gp.mu,gp=gp)
    # x0s = x0s[order(y0s)[1:5],]
    # for( i in 1:nrow(x0s) ) {
    #   x0 = x0s[i,]
    #   res = optim(par=x0,fn=gp.mu,gr=NULL,method="L-BFGS-B",lower=0,upper=1,
    #               control=list(trace=0,factr=10^-dgt),
    #               gp=gp)
    #   if( i==1 || res$value<res.best$value )
    #     res.best = res
    # }
    
    xc = xc.prev = rep(0.5,gp@d)
    notImproved = 0
    len = 1
    while( notImproved<5 && len>=0.01 ) {
      x0s = round(maximinLHS(100*gp@d,gp@d),dgt) * len + xc-len/2
      x0s = rbind(xc,x0s)
      x0s[which(x0s<0)] = 0; x0s[which(x0s>1)] = 1
      y0s = apply(x0s,1,gp.mu,gp=gp)
      xc = x0s[which.min(y0s),]
      len = 3 * len/4
      if( all(xc==xc.prev) ) {
        notImproved = notImproved+1
      } else {
        xc.prev = xc
        notImproved = 0
      }
    }
    res.best = list(par=xc)
    
    info = "greedy"
    par = res.best$par
  }
  
  next.Srinivas = list(par=par,af.info=info,af.note=paste0("epsilon=",epsilon))
}

# Epsilon-greedy LCB / Random on Pareto Front
next.epsPareto <- function(gp,epsilon) {
  if( runif(1,0,1)<epsilon ) {
    # There is no need to sample from a Gamma distribution, as in "Randomised GP-UCB for BO",
    # According to Zilinskas & Calvin, GP-UCB is on the Pareto whichever is beta! Therefore
    # we sample uniformly at random beta in [0;36], that is sqrt(beta) in [0;6], according to
    # the different Gaussian distribution's confidence intervals
    
    beta = round(runif(1,0,36))
    
    # x0s = round(maximinLHS(100*gp@d,gp@d),dgt)
    # y0s = apply(x0s,1,lcb,gp=gp)
    # x0s = x0s[order(y0s)[1:5],]
    # for( i in 1:nrow(x0s) ) {
    #   x0 = x0s[i,]
    #   res = optim(par=x0,fn=lcb,gr=NULL,method="L-BFGS-B",lower=0,upper=1,
    #               control=list(trace=0,factr=10^-dgt),
    #               gp=gp,beta=beta)
    #   if( i==1 || res$value<res.best$value )
    #     res.best = res
    # }
    
    xc = xc.prev = rep(0.5,gp@d)
    notImproved = 0
    len = 1
    while( notImproved<5 && len>=0.01 ) {
      x0s = round(maximinLHS(100*gp@d,gp@d),dgt) * len + xc-len/2
      x0s = rbind(xc,x0s)
      x0s[which(x0s<0)] = 0; x0s[which(x0s>1)] = 1
      y0s = apply(x0s,1,lcb,gp=gp,beta=beta)
      xc = x0s[which.min(y0s),]
      len = 3 * len/4
      if( all(xc==xc.prev) ) {
        notImproved = notImproved+1
      } else {
        xc.prev = xc
        notImproved = 0
      }
    }
    res.best = list(par=xc)
    
    info = "random"
    par = res.best$par
  } else {
    # x0s = round(maximinLHS(100*gp@d,gp@d),dgt)
    # y0s = apply(x0s,1,gp.mu,gp=gp)
    # x0s = x0s[order(y0s)[1:5],]
    # for( i in 1:nrow(x0s) ) {
    #   x0 = x0s[i,]
    #   res = optim(par=x0,fn=mu,gr=NULL,method="L-BFGS-B",lower=0,upper=1,
    #               control=list(trace=0,factr=10^-dgt),
    #               gp=gp)
    #   if( i==1 || res$value<res.best$value )
    #     res.best = res
    # }
    
    xc = xc.prev = rep(0.5,gp@d)
    notImproved = 0
    len = 1
    while( notImproved<5 && len>=0.01 ) {
      x0s = round(maximinLHS(100*gp@d,gp@d),dgt) * len + xc-len/2
      x0s = rbind(xc,x0s)
      x0s[which(x0s<0)] = 0; x0s[which(x0s>1)] = 1
      y0s = apply(x0s,1,mu,gp=gp,beta=beta)
      xc = x0s[which.min(y0s),]
      len = 3 * len/4
      if( all(xc==xc.prev) ) {
        notImproved = notImproved+1
      } else {
        xc.prev = xc
        notImproved = 0
      }
    }
    res.best = list(par=xc)
    
    info = "greedy"
    par = res.best$par
  }

  next.Srinivas = list(par=par,af.info=info,af.note=paste0("epsilon=",epsilon))
}


# Randomised GP-CB
# next.randCB <- function(gp,bea.mu,beta.sd) {
#   
# }


# Master Confidence Bound
# next.masterCB <- function(gp,L,nu) {
# 
# }


# GP-CB with constant beta
next.CB <- function(gp,beta) {
  # x0s = round(maximinLHS(100*gp@d,gp@d),dgt)
  # y0s = apply(x0s,1,lcb,gp=gp,beta=beta)
  # x0s = x0s[order(y0s)[1:5],]
  # for( i in 1:nrow(x0s) ) {
  #   x0 = x0s[i,]
  #   res = optim(par=x0,fn=lcb,gr=NULL,method="L-BFGS-B",lower=0,upper=1,
  #               control=list(trace=0,factr=10^-dgt),
  #               gp=gp,beta=beta)
  #   if( i==1 || res$value<res.best$value )
  #     res.best = res
  # }
  
  xc = xc.prev = rep(0.5,gp@d)
  notImproved = 0
  len = 1
  while( notImproved<5 && len>=0.01 ) {
    x0s = round(maximinLHS(100*gp@d,gp@d),dgt) * len + xc-len/2
    x0s = rbind(xc,x0s)
    x0s[which(x0s<0)] = 0; x0s[which(x0s>1)] = 1
    y0s = apply(x0s,1,lcb,gp=gp,beta=beta)
    xc = x0s[which.min(y0s),]
    len = 3 * len/4
    if( all(xc==xc.prev) ) {
      notImproved = notImproved+1
    } else {
      xc.prev = xc
      notImproved = 0
    }
  }
  res.best = list(par=xc)
  
  info = paste0("beta=",beta)
  par = res.best$par

  next.Srinivas = list(par=res.best$par,af.info=info,af.note=NA)
}


# Alternating between EI and PI
next.PIisBack.alternating <- function(gp,dgt) {

  xc = xc.prev = rep(0.5,gp@d)
  notImproved = 0
  len = 1
  while( notImproved<5 && len>=0.01 ) {
    x0s = round(maximinLHS(100*gp@d,gp@d),dgt) * len + xc-len/2
    x0s = rbind(xc,x0s)
    x0s[which(x0s<0)] = 0; x0s[which(x0s>1)] = 1
    
    if( nrow(gp@X)%%2==0 ) {
      # EI
      y0s = apply(x0s,1,EI,gp=gp,dgt=dgt)
      info = "EI"
    } else { 
      # PI
      y0s = apply(x0s,1,PI,gp=gp)
      info = "PI"
    }
    
    xc = x0s[which.min(y0s),]
    len = 3 * len/4
    if( all(xc==xc.prev) ) {
      notImproved = notImproved+1
    } else {
      xc.prev = xc
      notImproved = 0
    }
  }
  res.best = list(par=xc)
  
  par = res.best$par  

  next.Srinivas = list(par=res.best$par,af.info=info,af.note=NA)
}



# Switching from EI to PI
next.PIisBack.switching <- function(gp,N,rate,dgt) {
  
  xc = xc.prev = rep(0.5,gp@d)
  notImproved = 0
  len = 1
  while( notImproved<5 && len>=0.01 ) {
    x0s = round(maximinLHS(100*gp@d,gp@d),dgt) * len + xc-len/2
    x0s = rbind(xc,x0s)
    x0s[which(x0s<0)] = 0; x0s[which(x0s>1)] = 1
    
    if( nrow(gp@X)<=floor(N*rate) ) {
      # EI
      y0s = apply(x0s,1,EI,gp=gp,dgt=dgt)
      info = "EI"
    } else { 
      # PI
      y0s = apply(x0s,1,PI,gp=gp)
      info = "PI"
    }
    
    xc = x0s[which.min(y0s),]
    len = 3 * len/4
    if( all(xc==xc.prev) ) {
      notImproved = notImproved+1
    } else {
      xc.prev = xc
      notImproved = 0
    }
  }
  res.best = list(par=xc)
  
  par = res.best$par  
  
  next.Srinivas = list(par=res.best$par,af.info=info,af.note=NA)
}


# Alternating between CB and uncertainty (i.e. Inverse Weighted Distance ) reduction
# next.CB_UR.alternating <- function(gp) {
#   
# }

# Switching from uncertainty (i.e. Inverse Weighted Distance ) reduction to CB
# next.CB_UR.switching <- function(gp,rate) {
#   
# }

